# Owner(s): ["module: autograd"]

from torch.testing._internal.common_utils import TestCase, run_tests
import pkgutil
import torch

class TestPublicBindings(TestCase):
    def test_no_new_bindings(self):
        """
        This test aims to stop the introduction of new JIT bindings into torch._C
        whose names do not start with _. Such bindings are made available as
        torch.XXX, which may not be desirable.

        If your change causes this test to fail, add your new binding to a relevant
        submodule of torch._C, such as torch._C._jit (or other relevant submodule of
        torch._C). If your binding really needs to be available as torch.XXX, add it
        to torch._C and add it to the allowlist below.

        If you have removed a binding, remove it from the allowlist as well.
        """
        # This allowlist contains every binding in torch._C that is copied into torch at
        # the time of writing. It was generated with
        #
        #   {elem for elem in dir(torch._C) if not elem.startswith("_")}
        #
        torch_C_allowlist_superset = {
            "AggregationType",
            "AliasDb",
            "AnyType",
            "Argument",
            "ArgumentSpec",
            "autocast_decrement_nesting",
            "autocast_increment_nesting",
            "AVG",
            "BenchmarkConfig",
            "BenchmarkExecutionStats",
            "BFloat16StorageBase",
            "Block",
            "BoolStorageBase",
            "BoolType",
            "BufferDict",
            "ByteStorageBase",
            "CallStack",
            "Capsule",
            "CharStorageBase",
            "ClassType",
            "clear_autocast_cache",
            "Code",
            "CompilationUnit",
            "CompleteArgumentSpec",
            "ComplexDoubleStorageBase",
            "ComplexFloatStorageBase",
            "ComplexType",
            "ConcreteModuleType",
            "ConcreteModuleTypeBuilder",
            "CONV_BN_FUSION",
            "cpp",
            "CudaBFloat16StorageBase",
            "CudaBFloat16TensorBase",
            "CudaBFloat16TensorBase",
            "CudaBoolStorageBase",
            "CudaBoolTensorBase",
            "CudaBoolTensorBase",
            "CudaByteStorageBase",
            "CudaByteTensorBase",
            "CudaByteTensorBase",
            "CudaCharStorageBase",
            "CudaCharTensorBase",
            "CudaCharTensorBase",
            "CudaComplexDoubleStorageBase",
            "CudaComplexDoubleTensorBase",
            "CudaComplexDoubleTensorBase",
            "CudaComplexFloatStorageBase",
            "CudaComplexFloatTensorBase",
            "CudaComplexFloatTensorBase",
            "CudaDoubleStorageBase",
            "CudaDoubleTensorBase",
            "CudaDoubleTensorBase",
            "CudaFloatStorageBase",
            "CudaFloatTensorBase",
            "CudaHalfStorageBase",
            "CudaHalfTensorBase",
            "CudaIntStorageBase",
            "CudaIntTensorBase",
            "CudaIntTensorBase",
            "CudaLongStorageBase",
            "CudaLongTensorBase",
            "CudaLongTensorBase",
            "CudaShortStorageBase",
            "CudaShortTensorBase",
            "CudaShortTensorBase",
            "DeepCopyMemoTable",
            "default_generator",
            "DeserializationStorageContext",
            "device",
            "DeviceObjType",
            "DictType",
            "DisableTorchFunction",
            "DoubleStorageBase",
            "dtype",
            "EnumType",
            "ErrorReport",
            "ExecutionPlan",
            "FatalError",
            "FileCheck",
            "finfo",
            "FloatStorageBase",
            "FloatType",
            "fork",
            "FunctionSchema",
            "FUSE_ADD_RELU",
            "Future",
            "FutureType",
            "Generator",
            "get_autocast_cpu_dtype",
            "get_default_dtype",
            "get_num_interop_threads",
            "get_num_threads",
            "Gradient",
            "Graph",
            "GraphExecutorState",
            "HalfStorageBase",
            "has_cuda",
            "has_cudnn",
            "has_lapack",
            "has_mkl",
            "has_mkldnn",
            "has_mlc",
            "has_openmp",
            "has_spectral",
            "HOIST_CONV_PACKED_PARAMS",
            "iinfo",
            "import_ir_module_from_buffer",
            "import_ir_module",
            "InferredType",
            "init_num_threads",
            "INSERT_FOLD_PREPACK_OPS",
            "InterfaceType",
            "IntStorageBase",
            "IntType",
            "IODescriptor",
            "is_anomaly_enabled",
            "is_autocast_cache_enabled",
            "is_autocast_cpu_enabled",
            "is_autocast_enabled",
            "is_grad_enabled",
            "is_inference_mode_enabled",
            "JITException",
            "layout",
            "ListType",
            "LiteScriptModule",
            "LockingLogger",
            "LoggerBase",
            "LongStorageBase",
            "memory_format",
            "merge_type_from_type_comment",
            "MobileOptimizerType",
            "ModuleDict",
            "Node",
            "NoneType",
            "NoopLogger",
            "NumberType",
            "OperatorInfo",
            "OptionalType",
            "ParameterDict",
            "parse_ir",
            "parse_schema",
            "parse_type_comment",
            "PyObjectType",
            "PyTorchFileReader",
            "PyTorchFileWriter",
            "QInt32StorageBase",
            "QInt8StorageBase",
            "qscheme",
            "QUInt4x2StorageBase",
            "QUInt2x4StorageBase",
            "QUInt8StorageBase",
            "read_vitals",
            "REMOVE_DROPOUT",
            "RRefType",
            "ScriptClass",
            "ScriptClassFunction",
            "ScriptDict",
            "ScriptDictIterator",
            "ScriptDictKeyIterator",
            "ScriptList",
            "ScriptListIterator",
            "ScriptFunction",
            "ScriptMethod",
            "ScriptModule",
            "ScriptModuleSerializer",
            "ScriptObject",
            "ScriptObjectProperty",
            "SerializationStorageContext",
            "set_anomaly_enabled",
            "set_autocast_cache_enabled",
            "set_autocast_cpu_dtype",
            "set_autocast_cpu_enabled",
            "set_autocast_enabled",
            "set_flush_denormal",
            "set_num_interop_threads",
            "set_num_threads",
            "set_vital",
            "ShortStorageBase",
            "Size",
            "StaticModule",
            "Stream",
            "StreamObjType",
            "StringType",
            "SUM",
            "TensorType",
            "ThroughputBenchmark",
            "TracingState",
            "TupleType",
            "Type",
            "unify_type_list",
            "UnionType",
            "Use",
            "Value",
            "autocast_decrement_nesting",
            "autocast_increment_nesting",
            "clear_autocast_cache",
            "cpp",
            "default_generator",
            "device",
            "dtype",
            "finfo",
            "fork",
            "get_default_dtype",
            "get_num_interop_threads",
            "get_num_threads",
            "has_cuda",
            "has_cudnn",
            "has_lapack",
            "has_mkl",
            "has_mkldnn",
            "has_mlc",
            "has_openmp",
            "iinfo",
            "import_ir_module",
            "import_ir_module_from_buffer",
            "init_num_threads",
            "is_anomaly_enabled",
            "is_autocast_enabled",
            "is_grad_enabled",
            "layout",
            "memory_format",
            "merge_type_from_type_comment",
            "parse_ir",
            "parse_schema",
            "parse_type_comment",
            "qscheme",
            "set_anomaly_enabled",
            "set_autocast_enabled",
            'set_autocast_gpu_dtype',
            'get_autocast_gpu_dtype',
            "set_flush_denormal",
            "set_num_interop_threads",
            "set_num_threads",
            "unify_type_list",
            "vitals_enabled",

            "wait",
        }
        torch_C_bindings = {elem for elem in dir(torch._C) if not elem.startswith("_")}

        # Check that the torch._C bindings are all in the allowlist. Since
        # bindings can change based on how PyTorch was compiled (e.g. with/without
        # CUDA), the two may not be an exact match but the bindings should be
        # a subset of the allowlist.
        difference = torch_C_bindings.difference(torch_C_allowlist_superset)
        msg = f"torch._C had bindings that are not present in the allowlist:\n{difference}"
        self.assertTrue(torch_C_bindings.issubset(torch_C_allowlist_superset), msg)

    def test_correct_module_names(self):
        '''
        An API is considered public, if  its  `__module__` starts with `torch.`
        and there is no name in `__module__` or the object itself that starts with “_”.
        Each public package should either:
        (preferred) Define `__all__` and all non-module objects in there must have their `__module__` start with the current submodule's path.
        Things not in `__all__` should NOT have their `__module__` start with the current submodule.
        (for simple python-only modules) define `__all__` and all the elements in `dir(submod)` must have their `__module__` that start with the current submodule.
        '''
        allowlist_modules_without_all = {
            "torch.ao",
            "torch.ao.nn",
            "torch.ao.nn.sparse",
            "torch.ao.nn.sparse.quantized",
            "torch.ao.nn.sparse.quantized.dynamic",
            "torch.ao.nn.sparse.quantized.dynamic.linear",
            "torch.ao.nn.sparse.quantized.linear",
            "torch.ao.nn.sparse.quantized.utils",
            "torch.ao.ns",
            "torch.ao.ns.fx",
            "torch.ao.ns.fx.graph_matcher",
            "torch.ao.ns.fx.graph_passes",
            "torch.ao.ns.fx.mappings",
            "torch.ao.ns.fx.ns_types",
            "torch.ao.ns.fx.pattern_utils",
            "torch.ao.ns.fx.utils",
            "torch.ao.ns.fx.weight_utils",
            "torch.ao.quantization",
            "torch.ao.quantization.fake_quantize",
            "torch.ao.quantization.fuse_modules",
            "torch.ao.quantization.fuser_method_mappings",
            "torch.ao.quantization.fx",
            "torch.ao.quantization.fx.backend_config",
            "torch.ao.quantization.fx.backend_config.fuse_handler",
            "torch.ao.quantization.fx.backend_config.observation_type",
            "torch.ao.quantization.fx.backend_config.quantize_handler",
            "torch.ao.quantization.fx.backend_config.tensorrt",
            "torch.ao.quantization.fx.backend_config.utils",
            "torch.ao.quantization.fx.common_quantization_patterns",
            "torch.ao.quantization.fx.convert",
            "torch.ao.quantization.fx.fuse",
            "torch.ao.quantization.fx.fusion_patterns",
            "torch.ao.quantization.fx.graph_module",
            "torch.ao.quantization.fx.lower_to_fbgemm",
            "torch.ao.quantization.fx.lower_to_qnnpack",
            "torch.ao.quantization.fx.match_utils",
            "torch.ao.quantization.fx.pattern_utils",
            "torch.ao.quantization.fx.prepare",
            "torch.ao.quantization.fx.qconfig_utils",
            "torch.ao.quantization.fx.quantization_patterns",
            "torch.ao.quantization.fx.quantization_types",
            "torch.ao.quantization.fx.quantized_fusion_patterns_and_replacements",
            "torch.ao.quantization.fx.subgraph_rewriter_FORKED_DO_NOT_USE",
            "torch.ao.quantization.fx.utils",
            "torch.ao.quantization.observer",
            "torch.ao.quantization.qconfig",
            "torch.ao.quantization.qconfig_dict_utils",
            "torch.ao.quantization.quant_type",
            "torch.ao.quantization.quantization_mappings",
            "torch.ao.quantization.quantize",
            "torch.ao.quantization.quantize_fx",
            "torch.ao.quantization.quantize_jit",
            "torch.ao.quantization.stubs",
            "torch.ao.quantization.utils",
            "torch.ao.sparsity",
            "torch.ao.sparsity.experimental",
            "torch.ao.sparsity.experimental.pruner",
            "torch.ao.sparsity.experimental.pruner.base_pruner",
            "torch.ao.sparsity.experimental.pruner.parametrization",
            "torch.ao.sparsity.scheduler",
            "torch.ao.sparsity.scheduler.base_scheduler",
            "torch.ao.sparsity.scheduler.lambda_scheduler",
            "torch.ao.sparsity.sparsifier",
            "torch.ao.sparsity.sparsifier.base_sparsifier",
            "torch.ao.sparsity.sparsifier.utils",
            "torch.ao.sparsity.sparsifier.weight_norm_sparsifier",
            "torch.autocast_mode",
            "torch.autograd",
            "torch.autograd.anomaly_mode",
            "torch.autograd.forward_ad",
            "torch.autograd.function",
            "torch.autograd.functional",
            "torch.autograd.grad_mode",
            "torch.autograd.gradcheck",
            "torch.autograd.graph",
            "torch.autograd.profiler",
            "torch.autograd.profiler_legacy",
            "torch.autograd.profiler_util",
            "torch.autograd.variable",
            "torch.backends",
            "torch.backends.cuda",
            "torch.backends.cudnn",
            "torch.backends.cudnn.rnn",
            "torch.backends.mkl",
            "torch.backends.mkldnn",
            "torch.backends.openmp",
            "torch.backends.quantized",
            "torch.backends.xnnpack",
            "torch.contrib",
            "torch.cpu",
            "torch.cpu.amp",
            "torch.cpu.amp.autocast_mode",
            "torch.cuda",
            "torch.cuda.amp",
            "torch.cuda.amp.autocast_mode",
            "torch.cuda.amp.common",
            "torch.cuda.amp.grad_scaler",
            "torch.cuda.comm",
            "torch.cuda.error",
            "torch.cuda.graphs",
            "torch.cuda.memory",
            "torch.cuda.nccl",
            "torch.cuda.nvtx",
            "torch.cuda.profiler",
            "torch.cuda.random",
            "torch.cuda.sparse",
            "torch.cuda.streams",
            "torch.distributed",
            "torch.distributed.algorithms",
            "torch.distributed.algorithms.ddp_comm_hooks",
            "torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook",
            "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks",
            "torch.distributed.algorithms.ddp_comm_hooks.default_hooks",
            "torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks",
            "torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook",
            "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook",
            "torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks",
            "torch.distributed.algorithms.join",
            "torch.distributed.algorithms.model_averaging",
            "torch.distributed.algorithms.model_averaging.averagers",
            "torch.distributed.algorithms.model_averaging.hierarchical_model_averager",
            "torch.distributed.algorithms.model_averaging.utils",
            "torch.distributed.argparse_util",
            "torch.distributed.autograd",
            "torch.distributed.constants",
            "torch.distributed.distributed_c10d",
            "torch.distributed.elastic",
            "torch.distributed.elastic.agent",
            "torch.distributed.elastic.agent.server",
            "torch.distributed.elastic.agent.server.api",
            "torch.distributed.elastic.agent.server.local_elastic_agent",
            "torch.distributed.elastic.events",
            "torch.distributed.elastic.events.api",
            "torch.distributed.elastic.events.handlers",
            "torch.distributed.elastic.metrics",
            "torch.distributed.elastic.metrics.api",
            "torch.distributed.elastic.multiprocessing",
            "torch.distributed.elastic.multiprocessing.api",
            "torch.distributed.elastic.multiprocessing.errors",
            "torch.distributed.elastic.multiprocessing.errors.error_handler",
            "torch.distributed.elastic.multiprocessing.errors.handlers",
            "torch.distributed.elastic.multiprocessing.redirects",
            "torch.distributed.elastic.multiprocessing.tail_log",
            "torch.distributed.elastic.rendezvous",
            "torch.distributed.elastic.rendezvous.api",
            "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend",
            "torch.distributed.elastic.rendezvous.dynamic_rendezvous",
            "torch.distributed.elastic.rendezvous.etcd_rendezvous",
            "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend",
            "torch.distributed.elastic.rendezvous.etcd_server",
            "torch.distributed.elastic.rendezvous.etcd_store",
            "torch.distributed.elastic.rendezvous.registry",
            "torch.distributed.elastic.rendezvous.static_tcp_rendezvous",
            "torch.distributed.elastic.rendezvous.utils",
            "torch.distributed.elastic.timer",
            "torch.distributed.elastic.timer.api",
            "torch.distributed.elastic.timer.local_timer",
            "torch.distributed.elastic.utils",
            "torch.distributed.elastic.utils.api",
            "torch.distributed.elastic.utils.data",
            "torch.distributed.elastic.utils.data.cycling_iterator",
            "torch.distributed.elastic.utils.data.elastic_distributed_sampler",
            "torch.distributed.elastic.utils.distributed",
            "torch.distributed.elastic.utils.log_level",
            "torch.distributed.elastic.utils.logging",
            "torch.distributed.elastic.utils.store",
            "torch.distributed.fsdp",
            "torch.distributed.fsdp.flatten_params_wrapper",
            "torch.distributed.fsdp.fully_sharded_data_parallel",
            "torch.distributed.fsdp.utils",
            "torch.distributed.fsdp.wrap",
            "torch.distributed.launch",
            "torch.distributed.launcher",
            "torch.distributed.launcher.api",
            "torch.distributed.nn",
            "torch.distributed.nn.api",
            "torch.distributed.nn.api.remote_module",
            "torch.distributed.nn.functional",
            "torch.distributed.nn.jit",
            "torch.distributed.nn.jit.instantiator",
            "torch.distributed.nn.jit.templates",
            "torch.distributed.nn.jit.templates.remote_module_template",
            "torch.distributed.optim",
            "torch.distributed.optim.functional_adadelta",
            "torch.distributed.optim.functional_adagrad",
            "torch.distributed.optim.functional_adam",
            "torch.distributed.optim.functional_adamax",
            "torch.distributed.optim.functional_adamw",
            "torch.distributed.optim.functional_rmsprop",
            "torch.distributed.optim.functional_rprop",
            "torch.distributed.optim.functional_sgd",
            "torch.distributed.optim.optimizer",
            "torch.distributed.optim.post_localSGD_optimizer",
            "torch.distributed.optim.utils",
            "torch.distributed.optim.zero_redundancy_optimizer",
            "torch.distributed.pipeline",
            "torch.distributed.pipeline.sync",
            "torch.distributed.pipeline.sync.batchnorm",
            "torch.distributed.pipeline.sync.checkpoint",
            "torch.distributed.pipeline.sync.copy",
            "torch.distributed.pipeline.sync.dependency",
            "torch.distributed.pipeline.sync.microbatch",
            "torch.distributed.pipeline.sync.phony",
            "torch.distributed.pipeline.sync.pipe",
            "torch.distributed.pipeline.sync.pipeline",
            "torch.distributed.pipeline.sync.skip",
            "torch.distributed.pipeline.sync.skip.layout",
            "torch.distributed.pipeline.sync.skip.namespace",
            "torch.distributed.pipeline.sync.skip.portal",
            "torch.distributed.pipeline.sync.skip.skippable",
            "torch.distributed.pipeline.sync.skip.tracker",
            "torch.distributed.pipeline.sync.stream",
            "torch.distributed.pipeline.sync.utils",
            "torch.distributed.pipeline.sync.worker",
            "torch.distributed.remote_device",
            "torch.distributed.rendezvous",
            "torch.distributed.rpc",
            "torch.distributed.rpc.api",
            "torch.distributed.rpc.backend_registry",
            "torch.distributed.rpc.constants",
            "torch.distributed.rpc.functions",
            "torch.distributed.rpc.internal",
            "torch.distributed.rpc.options",
            "torch.distributed.rpc.rref_proxy",
            "torch.distributed.rpc.server_process_global_profiler",
            "torch.distributed.run",
            "torch.distributions",
            "torch.distributions.bernoulli",
            "torch.distributions.beta",
            "torch.distributions.binomial",
            "torch.distributions.categorical",
            "torch.distributions.cauchy",
            "torch.distributions.chi2",
            "torch.distributions.constraint_registry",
            "torch.distributions.constraints",
            "torch.distributions.continuous_bernoulli",
            "torch.distributions.dirichlet",
            "torch.distributions.distribution",
            "torch.distributions.exp_family",
            "torch.distributions.exponential",
            "torch.distributions.fishersnedecor",
            "torch.distributions.gamma",
            "torch.distributions.geometric",
            "torch.distributions.gumbel",
            "torch.distributions.half_cauchy",
            "torch.distributions.half_normal",
            "torch.distributions.independent",
            "torch.distributions.kl",
            "torch.distributions.kumaraswamy",
            "torch.distributions.laplace",
            "torch.distributions.lkj_cholesky",
            "torch.distributions.log_normal",
            "torch.distributions.logistic_normal",
            "torch.distributions.lowrank_multivariate_normal",
            "torch.distributions.mixture_same_family",
            "torch.distributions.multinomial",
            "torch.distributions.multivariate_normal",
            "torch.distributions.negative_binomial",
            "torch.distributions.normal",
            "torch.distributions.one_hot_categorical",
            "torch.distributions.pareto",
            "torch.distributions.poisson",
            "torch.distributions.relaxed_bernoulli",
            "torch.distributions.relaxed_categorical",
            "torch.distributions.studentT",
            "torch.distributions.transformed_distribution",
            "torch.distributions.transforms",
            "torch.distributions.uniform",
            "torch.distributions.utils",
            "torch.distributions.von_mises",
            "torch.distributions.weibull",
            "torch.distributions.wishart",
            "torch.fft",
            "torch.for_onnx",
            "torch.functional",
            "torch.futures",
            "torch.fx",
            "torch.fx.annotate",
            "torch.fx.experimental",
            "torch.fx.experimental.accelerator_partitioner",
            "torch.fx.experimental.const_fold",
            "torch.fx.experimental.debug",
            "torch.fx.experimental.graph_gradual_typechecker",
            "torch.fx.experimental.merge_matmul",
            "torch.fx.experimental.normalize",
            "torch.fx.experimental.optimization",
            "torch.fx.experimental.partitioner_utils",
            "torch.fx.experimental.refinement_types",
            "torch.fx.experimental.rewriter",
            "torch.fx.experimental.schema_type_annotation",
            "torch.fx.experimental.unification",
            "torch.fx.experimental.unification.core",
            "torch.fx.experimental.unification.dispatch",
            "torch.fx.experimental.unification.match",
            "torch.fx.experimental.unification.more",
            "torch.fx.experimental.unification.multipledispatch",
            "torch.fx.experimental.unification.multipledispatch.conflict",
            "torch.fx.experimental.unification.multipledispatch.core",
            "torch.fx.experimental.unification.multipledispatch.dispatcher",
            "torch.fx.experimental.unification.multipledispatch.utils",
            "torch.fx.experimental.unification.multipledispatch.variadic",
            "torch.fx.experimental.unification.unification_tools",
            "torch.fx.experimental.unification.utils",
            "torch.fx.experimental.unification.variable",
            "torch.fx.experimental.unify_refinements",
            "torch.fx.graph",
            "torch.fx.graph_module",
            "torch.fx.immutable_collections",
            "torch.fx.interpreter",
            "torch.fx.node",
            "torch.fx.operator_schemas",
            "torch.fx.passes",
            "torch.fx.passes.graph_drawer",
            "torch.fx.passes.graph_manipulation",
            "torch.fx.passes.net_min_base",
            "torch.fx.passes.operator_support",
            "torch.fx.passes.param_fetch",
            "torch.fx.passes.shape_prop",
            "torch.fx.passes.split_module",
            "torch.fx.passes.split_utils",
            "torch.fx.passes.splitter_base",
            "torch.fx.passes.tools_common",
            "torch.fx.proxy",
            "torch.fx.subgraph_rewriter",
            "torch.fx.tensor_type",
            "torch.hub",
            "torch.jit",
            "torch.jit.annotations",
            "torch.jit.frontend",
            "torch.jit.generate_bytecode",
            "torch.jit.mobile",
            "torch.jit.quantized",
            "torch.jit.supported_ops",
            "torch.jit.unsupported_tensor_ops",
            "torch.linalg",
            "torch.monitor",
            "torch.multiprocessing",
            "torch.multiprocessing.pool",
            "torch.multiprocessing.queue",
            "torch.multiprocessing.reductions",
            "torch.multiprocessing.spawn",
            "torch.nested",
            "torch.nn",
            "torch.nn.backends",
            "torch.nn.backends.thnn",
            "torch.nn.common_types",
            "torch.nn.cpp",
            "torch.nn.functional",
            "torch.nn.grad",
            "torch.nn.init",
            "torch.nn.intrinsic",
            "torch.nn.intrinsic.modules",
            "torch.nn.intrinsic.modules.fused",
            "torch.nn.intrinsic.qat",
            "torch.nn.intrinsic.qat.modules",
            "torch.nn.intrinsic.qat.modules.conv_fused",
            "torch.nn.intrinsic.qat.modules.linear_fused",
            "torch.nn.intrinsic.qat.modules.linear_relu",
            "torch.nn.intrinsic.quantized",
            "torch.nn.intrinsic.quantized.dynamic",
            "torch.nn.intrinsic.quantized.dynamic.modules",
            "torch.nn.intrinsic.quantized.dynamic.modules.linear_relu",
            "torch.nn.intrinsic.quantized.modules",
            "torch.nn.intrinsic.quantized.modules.bn_relu",
            "torch.nn.intrinsic.quantized.modules.conv_relu",
            "torch.nn.intrinsic.quantized.modules.linear_relu",
            "torch.nn.modules",
            "torch.nn.modules.activation",
            "torch.nn.modules.adaptive",
            "torch.nn.modules.batchnorm",
            "torch.nn.modules.channelshuffle",
            "torch.nn.modules.container",
            "torch.nn.modules.conv",
            "torch.nn.modules.distance",
            "torch.nn.modules.dropout",
            "torch.nn.modules.flatten",
            "torch.nn.modules.fold",
            "torch.nn.modules.instancenorm",
            "torch.nn.modules.lazy",
            "torch.nn.modules.linear",
            "torch.nn.modules.loss",
            "torch.nn.modules.module",
            "torch.nn.modules.normalization",
            "torch.nn.modules.padding",
            "torch.nn.modules.pixelshuffle",
            "torch.nn.modules.pooling",
            "torch.nn.modules.rnn",
            "torch.nn.modules.sparse",
            "torch.nn.modules.transformer",
            "torch.nn.modules.upsampling",
            "torch.nn.modules.utils",
            "torch.nn.parallel",
            "torch.nn.parallel.comm",
            "torch.nn.parallel.data_parallel",
            "torch.nn.parallel.distributed",
            "torch.nn.parallel.parallel_apply",
            "torch.nn.parallel.replicate",
            "torch.nn.parallel.scatter_gather",
            "torch.nn.parameter",
            "torch.nn.qat",
            "torch.nn.qat.dynamic",
            "torch.nn.qat.dynamic.modules",
            "torch.nn.qat.dynamic.modules.linear",
            "torch.nn.qat.modules",
            "torch.nn.qat.modules.conv",
            "torch.nn.qat.modules.embedding_ops",
            "torch.nn.qat.modules.linear",
            "torch.nn.quantizable",
            "torch.nn.quantizable.modules",
            "torch.nn.quantizable.modules.activation",
            "torch.nn.quantizable.modules.rnn",
            "torch.nn.quantized",
            "torch.nn.quantized.dynamic",
            "torch.nn.quantized.dynamic.modules",
            "torch.nn.quantized.dynamic.modules.conv",
            "torch.nn.quantized.dynamic.modules.linear",
            "torch.nn.quantized.dynamic.modules.rnn",
            "torch.nn.quantized.functional",
            "torch.nn.quantized.modules",
            "torch.nn.quantized.modules.activation",
            "torch.nn.quantized.modules.batchnorm",
            "torch.nn.quantized.modules.conv",
            "torch.nn.quantized.modules.dropout",
            "torch.nn.quantized.modules.embedding_ops",
            "torch.nn.quantized.modules.functional_modules",
            "torch.nn.quantized.modules.linear",
            "torch.nn.quantized.modules.normalization",
            "torch.nn.quantized.modules.utils",
            "torch.nn.utils",
            "torch.nn.utils.clip_grad",
            "torch.nn.utils.convert_parameters",
            "torch.nn.utils.fusion",
            "torch.nn.utils.init",
            "torch.nn.utils.memory_format",
            "torch.nn.utils.parametrizations",
            "torch.nn.utils.parametrize",
            "torch.nn.utils.prune",
            "torch.nn.utils.rnn",
            "torch.nn.utils.spectral_norm",
            "torch.nn.utils.weight_norm",
            "torch.onnx",
            "torch.onnx.operators",
            "torch.onnx.symbolic_caffe2",
            "torch.onnx.symbolic_helper",
            "torch.onnx.symbolic_opset10",
            "torch.onnx.symbolic_opset11",
            "torch.onnx.symbolic_opset12",
            "torch.onnx.symbolic_opset13",
            "torch.onnx.symbolic_opset14",
            "torch.onnx.symbolic_opset15",
            "torch.onnx.symbolic_opset7",
            "torch.onnx.symbolic_opset8",
            "torch.onnx.symbolic_opset9",
            "torch.onnx.symbolic_registry",
            "torch.onnx.utils",
            "torch.optim",
            "torch.optim.adadelta",
            "torch.optim.adagrad",
            "torch.optim.adam",
            "torch.optim.adamax",
            "torch.optim.adamw",
            "torch.optim.asgd",
            "torch.optim.lbfgs",
            "torch.optim.lr_scheduler",
            "torch.optim.nadam",
            "torch.optim.optimizer",
            "torch.optim.radam",
            "torch.optim.rmsprop",
            "torch.optim.rprop",
            "torch.optim.sgd",
            "torch.optim.sparse_adam",
            "torch.optim.swa_utils",
            "torch.overrides",
            "torch.package",
            "torch.package.analyze",
            "torch.package.analyze.is_from_package",
            "torch.package.analyze.trace_dependencies",
            "torch.package.file_structure_representation",
            "torch.package.find_file_dependencies",
            "torch.package.glob_group",
            "torch.package.importer",
            "torch.package.package_exporter",
            "torch.package.package_importer",
            "torch.profiler",
            "torch.profiler.profiler",
            "torch.profiler.python_tracer",
            "torch.quantization",
            "torch.quantization.fake_quantize",
            "torch.quantization.fuse_modules",
            "torch.quantization.fuser_method_mappings",
            "torch.quantization.fx",
            "torch.quantization.fx.convert",
            "torch.quantization.fx.fuse",
            "torch.quantization.fx.fusion_patterns",
            "torch.quantization.fx.graph_module",
            "torch.quantization.fx.match_utils",
            "torch.quantization.fx.pattern_utils",
            "torch.quantization.fx.prepare",
            "torch.quantization.fx.quantization_patterns",
            "torch.quantization.fx.quantization_types",
            "torch.quantization.fx.utils",
            "torch.quantization.observer",
            "torch.quantization.qconfig",
            "torch.quantization.quant_type",
            "torch.quantization.quantization_mappings",
            "torch.quantization.quantize",
            "torch.quantization.quantize_fx",
            "torch.quantization.quantize_jit",
            "torch.quantization.stubs",
            "torch.quantization.utils",
            "torch.quasirandom",
            "torch.random",
            "torch.return_types",
            "torch.serialization",
            "torch.sparse",
            "torch.special",
            "torch.storage",
            "torch.testing",
            "torch.torch_version",
            "torch.types",
            "torch.utils",
            "torch.utils.backcompat",
            "torch.utils.benchmark",
            "torch.utils.benchmark.examples",
            "torch.utils.benchmark.examples.blas_compare",
            "torch.utils.benchmark.examples.blas_compare_setup",
            "torch.utils.benchmark.examples.compare",
            "torch.utils.benchmark.examples.end_to_end",
            "torch.utils.benchmark.examples.fuzzer",
            "torch.utils.benchmark.examples.op_benchmark",
            "torch.utils.benchmark.examples.simple_timeit",
            "torch.utils.benchmark.examples.spectral_ops_fuzz_test",
            "torch.utils.benchmark.op_fuzzers",
            "torch.utils.benchmark.op_fuzzers.binary",
            "torch.utils.benchmark.op_fuzzers.sparse_binary",
            "torch.utils.benchmark.op_fuzzers.sparse_unary",
            "torch.utils.benchmark.op_fuzzers.spectral",
            "torch.utils.benchmark.op_fuzzers.unary",
            "torch.utils.benchmark.utils",
            "torch.utils.benchmark.utils.common",
            "torch.utils.benchmark.utils.compare",
            "torch.utils.benchmark.utils.cpp_jit",
            "torch.utils.benchmark.utils.fuzzer",
            "torch.utils.benchmark.utils.sparse_fuzzer",
            "torch.utils.benchmark.utils.timer",
            "torch.utils.benchmark.utils.valgrind_wrapper",
            "torch.utils.benchmark.utils.valgrind_wrapper.timer_interface",
            "torch.utils.bottleneck",
            "torch.utils.bundled_inputs",
            "torch.utils.checkpoint",
            "torch.utils.collect_env",
            "torch.utils.cpp_extension",
            "torch.utils.data",
            "torch.utils.data.backward_compatibility",
            "torch.utils.data.communication",
            "torch.utils.data.communication.eventloop",
            "torch.utils.data.communication.iter",
            "torch.utils.data.communication.map",
            "torch.utils.data.communication.messages",
            "torch.utils.data.communication.protocol",
            "torch.utils.data.communication.queue",
            "torch.utils.data.dataloader",
            "torch.utils.data.dataloader_experimental",
            "torch.utils.data.datapipes",
            "torch.utils.data.datapipes.dataframe",
            "torch.utils.data.datapipes.dataframe.dataframe_wrapper",
            "torch.utils.data.datapipes.dataframe.dataframes",
            "torch.utils.data.datapipes.dataframe.datapipes",
            "torch.utils.data.datapipes.dataframe.structures",
            "torch.utils.data.datapipes.iter",
            "torch.utils.data.datapipes.iter.callable",
            "torch.utils.data.datapipes.iter.combinatorics",
            "torch.utils.data.datapipes.iter.combining",
            "torch.utils.data.datapipes.iter.filelister",
            "torch.utils.data.datapipes.iter.fileopener",
            "torch.utils.data.datapipes.iter.grouping",
            "torch.utils.data.datapipes.iter.routeddecoder",
            "torch.utils.data.datapipes.iter.selecting",
            "torch.utils.data.datapipes.iter.streamreader",
            "torch.utils.data.datapipes.iter.utils",
            "torch.utils.data.datapipes.map",
            "torch.utils.data.datapipes.map.callable",
            "torch.utils.data.datapipes.map.combinatorics",
            "torch.utils.data.datapipes.map.combining",
            "torch.utils.data.datapipes.map.grouping",
            "torch.utils.data.datapipes.map.utils",
            "torch.utils.data.datapipes.utils",
            "torch.utils.data.datapipes.utils.common",
            "torch.utils.data.datapipes.utils.decoder",
            "torch.utils.data.dataset",
            "torch.utils.data.distributed",
            "torch.utils.data.gen_pyi",
            "torch.utils.data.graph",
            "torch.utils.data.graph_settings",
            "torch.utils.data.sampler",
            "torch.utils.dlpack",
            "torch.utils.ffi",
            "torch.utils.file_baton",
            "torch.utils.hipify",
            "torch.utils.hipify.constants",
            "torch.utils.hipify.cuda_to_hip_mappings",
            "torch.utils.hipify.hipify_python",
            "torch.utils.hipify.version",
            "torch.utils.hooks",
            "torch.utils.mkldnn",
            "torch.utils.mobile_optimizer",
            "torch.utils.model_dump",
            "torch.utils.model_zoo",
            "torch.utils.show_pickle",
            "torch.utils.tensorboard",
            "torch.utils.tensorboard.summary",
            "torch.utils.tensorboard.writer",
            "torch.utils.throughput_benchmark",
            "torch.version"
        }
        for _, modname, ispkg in pkgutil.walk_packages(path=torch.__path__, prefix=torch.__name__ + '.'):
            is_a_private_module = False
            split_strs = modname.split('.')
            for elem in split_strs:
                if elem.startswith("_"):
                    is_a_private_module = True
                    break

            if is_a_private_module:
                continue

            def module_starts_with_modname(elem, modname):
                private_elem = elem.startswith('_')
                obj = getattr(modname, elem)
                elem_module = getattr(obj, '__module__') if hasattr(obj, '__module__') else None
                elem_modname_starts_with_modname = elem_module is None or elem_module.startswith(modname)
                if private_elem:
                    assert elem_module is None or not elem_modname_starts_with_modname
                else:
                    assert elem_modname_starts_with_modname

            if hasattr(modname, '__all__'):
                assert not is_a_private_module
                public_api = getattr(modname, '__all__')
                all_api = dir(modname)
                for elem in all_api:
                    assert not (elem in public_api ^ module_starts_with_modname(elem, modname))
            else:
                assert modname in allowlist_modules_without_all
                all_api = dir(modname)
                for elem in all_api:
                    module_starts_with_modname(elem, modname)

if __name__ == '__main__':
    run_tests()
